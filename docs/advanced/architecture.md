# Architecture

MCPOmni Connect is built with a modular, extensible architecture designed for scalability, reliability, and ease of use. This document provides a comprehensive overview of the system's design and components.

## System Overview

MCPOmni Connect acts as an intelligent gateway between users and the Model Context Protocol (MCP) ecosystem, providing AI-powered automation and orchestration capabilities.

```mermaid
graph TB
    User[ðŸ‘¤ User] --> CLI[ðŸ–¥ï¸ CLI Interface]
    CLI --> Core[ðŸ§  Core Engine]
    
    Core --> LLM[ðŸ¤– LLM Integration]
    Core --> Memory[ðŸ’¾ Memory Management]
    Core --> Session[ðŸ”„ Session Management]
    Core --> Transport[ðŸš€ Transport Layer]
    
    LLM --> Providers[â˜ï¸ LLM Providers]
    Memory --> Redis[ðŸ“Š Redis]
    Memory --> Files[ðŸ“ File Storage]
    
    Transport --> Stdio[ðŸ“º Stdio]
    Transport --> SSE[ðŸ“¡ SSE]
    Transport --> HTTP[ðŸŒ HTTP]
    
    Stdio --> LocalMCP[ðŸ”§ Local MCP Servers]
    SSE --> RemoteMCP[ðŸŒ Remote MCP Servers]
    HTTP --> APIMnCP[ðŸ”Œ API MCP Servers]
    
    Core --> Agent[ðŸ¤– Agent System]
    Agent --> Chat[ðŸ’¬ Chat Mode]
    Agent --> Auto[âš¡ Autonomous Mode]
    Agent --> Orch[ðŸŽ¯ Orchestrator Mode]
```

## Core Components

### 1. CLI Interface Layer

The user-facing command-line interface that handles input/output and user interactions.

**Responsibilities:**
- Command parsing and validation
- User input handling
- Output formatting and display
- Interactive prompts and confirmations
- Error message presentation

**Key Features:**
- Rich text formatting with syntax highlighting
- Interactive command completion
- Real-time status updates
- Debug mode visualization

### 2. Core Engine

The central orchestrator that coordinates all system components.

**Responsibilities:**
- Component lifecycle management
- Event coordination and messaging
- Configuration management
- Error handling and recovery
- System state management

**Components:**
```python
class CoreEngine:
    def __init__(self):
        self.session_manager = SessionManager()
        self.transport_layer = TransportLayer()
        self.llm_integration = LLMIntegration()
        self.memory_manager = MemoryManager()
        self.agent_system = AgentSystem()
```

### 3. Agent System

The AI-powered decision-making and execution engine.

```mermaid
graph LR
    Agent[ðŸ§  Agent System] --> ReAct[ðŸ”„ ReAct Engine]
    Agent --> Orchestrator[ðŸŽ¯ Orchestrator]
    Agent --> Context[ðŸ“š Context Manager]
    
    ReAct --> Reasoning[ðŸ’­ Reasoning]
    ReAct --> Acting[âš¡ Acting]
    ReAct --> Observing[ðŸ‘ï¸ Observing]
    
    Orchestrator --> Planning[ðŸ“‹ Planning]
    Orchestrator --> Coordination[ðŸ¤ Coordination]
    Orchestrator --> Monitoring[ðŸ“Š Monitoring]
```

**Mode Architecture:**

=== "Chat Mode"
    ```python
    class ChatMode:
        def process_request(self, user_input):
            # 1. Parse user intent
            intent = self.parse_intent(user_input)
            
            # 2. Plan actions
            actions = self.plan_actions(intent)
            
            # 3. Request approval for each action
            for action in actions:
                if self.request_approval(action):
                    result = self.execute_action(action)
                    self.present_result(result)
    ```

=== "Autonomous Mode"
    ```python
    class AutonomousMode:
        def process_request(self, user_input):
            # 1. Parse and understand goal
            goal = self.parse_goal(user_input)
            
            # 2. ReAct loop
            while not self.goal_achieved(goal):
                thought = self.think(current_state)
                action = self.plan_action(thought)
                observation = self.execute_action(action)
                self.update_state(observation)
            
            # 3. Report completion
            return self.generate_report()
    ```

=== "Orchestrator Mode"
    ```python
    class OrchestratorMode:
        def process_request(self, user_input):
            # 1. Strategic analysis
            strategy = self.analyze_requirements(user_input)
            
            # 2. Multi-phase planning
            phases = self.create_execution_plan(strategy)
            
            # 3. Coordinate execution
            for phase in phases:
                agents = self.allocate_agents(phase)
                results = self.execute_parallel(agents)
                self.merge_results(results)
            
            return self.final_report()
    ```

## Transport Layer

### Transport Architecture

```mermaid
graph TB
    TL[ðŸš€ Transport Layer] --> TM[ðŸ“‹ Transport Manager]
    TM --> Registry[ðŸ“Š Transport Registry]
    TM --> Factory[ðŸ­ Transport Factory]
    
    Factory --> StdioT[ðŸ“º Stdio Transport]
    Factory --> SSET[ðŸ“¡ SSE Transport]
    Factory --> HTTPT[ðŸŒ HTTP Transport]
    
    StdioT --> Process[âš™ï¸ Process Manager]
    SSET --> EventStream[ðŸ“ˆ Event Stream]
    HTTPT --> AuthManager[ðŸ” Auth Manager]
    
    AuthManager --> OAuth[ðŸ”‘ OAuth Handler]
    AuthManager --> Bearer[ðŸŽ« Bearer Token]
    AuthManager --> Custom[ðŸ”§ Custom Headers]
```

### Transport Implementations

#### Stdio Transport
```python
class StdioTransport:
    def __init__(self, command, args):
        self.process = subprocess.Popen(
            [command] + args,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
    
    async def send_message(self, message):
        await self.process.stdin.write(message)
    
    async def receive_message(self):
        return await self.process.stdout.readline()
```

#### SSE Transport
```python
class SSETransport:
    def __init__(self, url, headers):
        self.url = url
        self.headers = headers
        self.client = httpx.AsyncClient()
    
    async def connect(self):
        self.stream = self.client.stream(
            "GET", self.url, headers=self.headers
        )
    
    async def receive_events(self):
        async for line in self.stream.aiter_lines():
            if line.startswith("data: "):
                yield json.loads(line[6:])
```

#### HTTP Transport
```python
class HTTPTransport:
    def __init__(self, url, auth_config):
        self.url = url
        self.auth = self.setup_auth(auth_config)
        self.client = httpx.AsyncClient()
    
    async def send_request(self, data):
        response = await self.client.post(
            self.url,
            json=data,
            headers=self.auth.get_headers()
        )
        return response.json()
```

## Session Management

### Session Architecture

```mermaid
graph LR
    SM[ðŸ”„ Session Manager] --> SL[ðŸ“Š Session Lifecycle]
    SM --> CR[ðŸ”— Connection Registry]
    SM --> HM[ðŸ’– Health Monitor]
    
    CR --> Servers[ðŸ–¥ï¸ Server Connections]
    HM --> Heartbeat[ðŸ’“ Heartbeat]
    HM --> Recovery[ðŸ”„ Recovery]
    
    Servers --> Active[âœ… Active]
    Servers --> Idle[ðŸ˜´ Idle]
    Servers --> Failed[âŒ Failed]
```

### Connection Management

```python
class SessionManager:
    def __init__(self):
        self.connections = {}
        self.health_monitor = HealthMonitor()
        self.recovery_manager = RecoveryManager()
    
    async def connect_server(self, server_config):
        transport = self.create_transport(server_config)
        connection = await transport.connect()
        
        self.connections[server_config.name] = connection
        self.health_monitor.add_connection(connection)
        
        return connection
    
    async def health_check(self):
        for name, connection in self.connections.items():
            if not await connection.is_healthy():
                await self.recovery_manager.recover(name, connection)
```

## LLM Integration

### LiteLLM Integration Architecture

```mermaid
graph TB
    LLM[ðŸ¤– LLM Integration] --> LiteLLM[âš¡ LiteLLM]
    LLM --> Config[âš™ï¸ Config Manager]
    LLM --> Context[ðŸ“š Context Manager]
    
    LiteLLM --> OpenAI[ðŸ”µ OpenAI]
    LiteLLM --> Anthropic[ðŸŸ£ Anthropic]
    LiteLLM --> Google[ðŸ”´ Google]
    LiteLLM --> Others[... Others]
    
    Context --> Window[ðŸªŸ Context Window]
    Context --> History[ðŸ“œ History]
    Context --> Pruning[âœ‚ï¸ Pruning]
```

### LLM Integration Implementation

```python
class LLMIntegration:
    def __init__(self, config):
        self.config = config
        self.context_manager = ContextManager()
        self.client = self.setup_litellm()
    
    def setup_litellm(self):
        return litellm.completion
    
    async def generate_response(self, messages, tools=None):
        # Prepare context
        context = self.context_manager.prepare_context(messages)
        
        # Call LLM
        response = await self.client(
            model=f"{self.config.provider}/{self.config.model}",
            messages=context,
            tools=tools,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens
        )
        
        return response
```

## Memory Management

### Memory Architecture

```mermaid
graph TB
    MM[ðŸ’¾ Memory System] --> IM[ðŸ§  InMemoryStore]
    MM --> RM[ðŸ“Š RedisShortTermMemory]
    MM --> File[ðŸ“ File Storage]
    
    IM --> Session[âš¡ Session Storage]
    IM --> Context[ðŸ“¦ Context Management]
    IM --> Truncation[âœ‚ï¸ Token Truncation]
    
    RM --> Persistence[ðŸ’¾ Redis Persistence]
    RM --> TTL[â° TTL Management]
    RM --> Serialization[ðŸ”„ Serialization]
    
    File --> Save[ðŸ’¾ Save Operations]
    File --> Load[ðŸ“¥ Load Operations]
    File --> Backup[ðŸ”„ Backup]
```

### Memory Implementation

```python
# InMemoryStore - Database compatible session memory
class InMemoryStore:
    def __init__(self, max_context_tokens=30000, debug=False):
        self.sessions_history = {}  # session_id -> messages
        self.max_context_tokens = max_context_tokens
        self.debug = debug
    
    async def store_message(self, role, content, metadata=None, session_id=None):
        # Store message with agent_name in metadata
        if metadata is None:
            metadata = {}
        
        message = {
            "role": role,
            "content": content,
            "session_id": session_id,
            "timestamp": time.time(),
            "metadata": metadata,
        }
        
        if session_id not in self.sessions_history:
            self.sessions_history[session_id] = []
        self.sessions_history[session_id].append(message)
    
    async def get_messages(self, session_id=None, agent_name=None):
        # Get messages for session, optionally filtered by agent
        if session_id not in self.sessions_history:
            return []
        
        messages = self.sessions_history[session_id]
        
        # Filter by agent_name if provided
        if agent_name:
            messages = [
                msg for msg in messages 
                if msg.get("metadata", {}).get("agent_name") == agent_name
            ]
        
        return messages

# RedisShortTermMemory - Persistent memory with Redis
class RedisShortTermMemory:
    def __init__(self, redis_client=None, max_context_tokens=30000):
        self._redis_client = redis_client or redis.Redis()
        self.max_context_tokens = max_context_tokens
    
    async def store_message(self, role, content, metadata=None):
        # Store in Redis with client ID
        key = f"mcp_memory:{self.client_id}"
        message = {
            "role": role,
            "content": content,
            "metadata": metadata,
            "timestamp": time.time(),
        }
        await self._redis_client.zadd(key, {json.dumps(message): time.time()})
    
    async def get_messages(self):
        # Retrieve from Redis
        key = f"mcp_memory:{self.client_id}"
        messages = await self._redis_client.zrange(key, 0, -1)
        return [json.loads(msg) for msg in messages]
```

## Tool Management

### Tool Discovery and Execution

```mermaid
graph LR
    TM[ðŸ”§ Tool Manager] --> Discovery[ðŸ” Discovery]
    TM --> Registry[ðŸ“‹ Registry]
    TM --> Executor[âš¡ Executor]
    
    Discovery --> Servers[ðŸ–¥ï¸ Server Tools]
    Registry --> Metadata[ðŸ“Š Tool Metadata]
    Registry --> Routing[ðŸ›¤ï¸ Routing Rules]
    
    Executor --> Parallel[âš¡ Parallel Exec]
    Executor --> Serial[ðŸ”„ Serial Exec]
    Executor --> Fallback[ðŸ”„ Fallback]
```

### Tool Execution Engine

```python
class ToolManager:
    def __init__(self):
        self.registry = ToolRegistry()
        self.executor = ToolExecutor()
        self.router = ToolRouter()
    
    async def discover_tools(self, connections):
        for connection in connections:
            tools = await connection.list_tools()
            for tool in tools:
                self.registry.register(tool, connection)
    
    async def execute_tool(self, tool_name, parameters):
        # Route to appropriate server
        connection = self.router.route(tool_name)
        
        # Execute with timeout and retry
        return await self.executor.execute(
            connection, tool_name, parameters
        )
```

## Security Architecture

### Security Layers

```mermaid
graph TB
    Security[ðŸ” Security] --> Auth[ðŸ”‘ Authentication]
    Security --> Authz[ðŸ›¡ï¸ Authorization]
    Security --> Encryption[ðŸ”’ Encryption]
    Security --> Isolation[ðŸ° Isolation]
    
    Auth --> OAuth[ðŸ” OAuth 2.0]
    Auth --> Tokens[ðŸŽ« Bearer Tokens]
    Auth --> Custom[ðŸ”§ Custom Auth]
    
    Authz --> ServerLevel[ðŸ–¥ï¸ Server Level]
    Authz --> ToolLevel[ðŸ”§ Tool Level]
    
    Encryption --> Transit[ðŸš€ In Transit]
    Encryption --> Rest[ðŸ’¾ At Rest]
    
    Isolation --> ServerIso[ðŸ  Server Isolation]
    Isolation --> DataIso[ðŸ“Š Data Isolation]
```

### Security Implementation

```python
class SecurityManager:
    def __init__(self):
        self.auth_manager = AuthenticationManager()
        self.authz_manager = AuthorizationManager()
        self.crypto = CryptographyManager()
    
    async def authenticate_server(self, server_config):
        if server_config.auth_method == "oauth":
            return await self.auth_manager.oauth_flow(server_config)
        elif server_config.auth_method == "bearer":
            return self.auth_manager.bearer_token(server_config)
        
    def encrypt_sensitive_data(self, data):
        return self.crypto.encrypt(data)
    
    def authorize_tool_access(self, tool, user_context):
        return self.authz_manager.check_permission(tool, user_context)
```

## Performance and Scalability

### Performance Architecture

```mermaid
graph LR
    Perf[âš¡ Performance] --> Caching[ðŸ—„ï¸ Caching]
    Perf --> Pooling[ðŸŠ Connection Pooling]
    Perf --> Async[ðŸ”„ Async Processing]
    Perf --> Monitoring[ðŸ“Š Monitoring]
    
    Caching --> ToolCache[ðŸ”§ Tool Results]
    Caching --> ContextCache[ðŸ“š Context Cache]
    
    Pooling --> ConnPool[ðŸ”— Connection Pool]
    Pooling --> ThreadPool[ðŸ§µ Thread Pool]
    
    Async --> EventLoop[ðŸ”„ Event Loop]
    Async --> Coroutines[âš¡ Coroutines]
```

### Performance Optimizations

```python
class PerformanceManager:
    def __init__(self):
        self.cache = CacheManager()
        self.connection_pool = ConnectionPool()
        self.metrics = MetricsCollector()
    
    async def execute_with_cache(self, tool_call):
        cache_key = self.generate_cache_key(tool_call)
        
        # Check cache first
        cached_result = await self.cache.get(cache_key)
        if cached_result:
            self.metrics.record_cache_hit(tool_call)
            return cached_result
        
        # Execute and cache result
        result = await self.execute_tool(tool_call)
        await self.cache.set(cache_key, result, ttl=300)
        
        self.metrics.record_cache_miss(tool_call)
        return result
```

## Configuration System

### Configuration Architecture

```mermaid
graph TB
    Config[âš™ï¸ Configuration] --> Env[ðŸŒ Environment]
    Config --> JSON[ðŸ“„ JSON Config]
    Config --> Runtime[âš¡ Runtime Config]
    
    Env --> APIKeys[ðŸ”‘ API Keys]
    Env --> Redis[ðŸ“Š Redis Config]
    Env --> Debug[ðŸ› Debug Settings]
    
    JSON --> LLMConfig[ðŸ¤– LLM Config]
    JSON --> Servers[ðŸ–¥ï¸ Server Config]
    JSON --> AgentConfig[ðŸ¤– Agent Config]
    
    Runtime --> Dynamic[ðŸ”„ Dynamic Updates]
    Runtime --> Validation[âœ… Validation]
```

### Configuration Management

```python
class ConfigurationManager:
    def __init__(self):
        self.env_config = self.load_env_config()
        self.json_config = self.load_json_config()
        self.runtime_config = {}
        self.validators = ConfigValidators()
    
    def load_env_config(self):
        return {
            'llm_api_key': os.getenv('LLM_API_KEY'),
            'redis_host': os.getenv('REDIS_HOST', 'localhost'),
            'redis_port': int(os.getenv('REDIS_PORT', 6379)),
            'debug': os.getenv('DEBUG', 'false').lower() == 'true'
        }
    
    def validate_configuration(self):
        errors = []
        
        # Validate environment variables
        if not self.env_config.get('llm_api_key'):
            errors.append("LLM_API_KEY is required")
        
        # Validate JSON configuration
        if not self.json_config.get('LLM'):
            errors.append("LLM configuration is required")
        
        if errors:
            raise ConfigurationError(errors)
```

## Error Handling and Recovery

### Error Handling Strategy

```mermaid
graph TB
    Error[âŒ Error Handling] --> Detection[ðŸ” Detection]
    Error --> Classification[ðŸ“Š Classification]
    Error --> Recovery[ðŸ”„ Recovery]
    Error --> Reporting[ðŸ“¢ Reporting]
    
    Detection --> Monitoring[ðŸ“Š Monitoring]
    Detection --> Logging[ðŸ“ Logging]
    
    Classification --> Transient[â±ï¸ Transient]
    Classification --> Permanent[ðŸ”’ Permanent]
    Classification --> Unknown[â“ Unknown]
    
    Recovery --> Retry[ðŸ”„ Retry]
    Recovery --> Fallback[ðŸ”„ Fallback]
    Recovery --> Graceful[âœ… Graceful Degradation]
```

### Recovery Implementation

```python
class ErrorRecoveryManager:
    def __init__(self):
        self.retry_policies = RetryPolicies()
        self.fallback_strategies = FallbackStrategies()
        self.circuit_breakers = CircuitBreakerRegistry()
    
    async def handle_error(self, error, context):
        error_type = self.classify_error(error)
        
        if error_type == ErrorType.TRANSIENT:
            return await self.retry_with_backoff(context)
        elif error_type == ErrorType.PERMANENT:
            return await self.execute_fallback(context)
        else:
            return await self.graceful_degradation(context)
    
    async def retry_with_backoff(self, context, max_retries=3):
        for attempt in range(max_retries):
            try:
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
                return await context.retry()
            except Exception as e:
                if attempt == max_retries - 1:
                    raise e
                continue
```

## Monitoring and Observability

### Observability Stack

```mermaid
graph LR
    Obs[ðŸ‘ï¸ Observability] --> Metrics[ðŸ“Š Metrics]
    Obs --> Logging[ðŸ“ Logging]
    Obs --> Tracing[ðŸ” Tracing]
    Obs --> Health[ðŸ’– Health Checks]
    
    Metrics --> Performance[âš¡ Performance]
    Metrics --> Usage[ðŸ“ˆ Usage]
    Metrics --> Errors[âŒ Errors]
    
    Logging --> Structured[ðŸ“‹ Structured]
    Logging --> Levels[ðŸ“Š Log Levels]
    
    Tracing --> Requests[ðŸ“ Request Tracing]
    Tracing --> Dependencies[ðŸ”— Dependency Tracing]
```

### Monitoring Implementation

```python
class MonitoringManager:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.logger = StructuredLogger()
        self.tracer = DistributedTracer()
        self.health_checker = HealthChecker()
    
    def record_tool_execution(self, tool_name, duration, success):
        self.metrics_collector.increment(
            'tool_executions_total',
            tags={'tool': tool_name, 'success': success}
        )
        self.metrics_collector.histogram(
            'tool_execution_duration',
            duration,
            tags={'tool': tool_name}
        )
    
    def log_user_interaction(self, user_input, response, context):
        self.logger.info(
            "user_interaction",
            user_input=user_input,
            response_length=len(response),
            mode=context.mode,
            servers_connected=len(context.servers)
        )
```

## Extensibility and Plugin System

### Plugin Architecture

```mermaid
graph TB
    Plugin[ðŸ”Œ Plugin System] --> Registry[ðŸ“‹ Plugin Registry]
    Plugin --> Loader[ðŸ“¥ Plugin Loader]
    Plugin --> Lifecycle[ðŸ”„ Lifecycle Manager]
    
    Registry --> Transport[ðŸš€ Transport Plugins]
    Registry --> LLM[ðŸ¤– LLM Plugins]
    Registry --> Tool[ðŸ”§ Tool Plugins]
    Registry --> Memory[ðŸ’¾ Memory Plugins]
    
    Loader --> Discovery[ðŸ” Discovery]
    Loader --> Validation[âœ… Validation]
    Loader --> Installation[ðŸ“¦ Installation]
```

This architecture provides a solid foundation for MCPOmni Connect's current capabilities while allowing for future expansion and customization.

---

**Next**: [API Reference â†’](api-reference.md) 